{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac389308-d479-4adb-8222-ab1f97b17bfc",
   "metadata": {},
   "source": [
    "# Orientações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e00ce1-5c6d-49cc-8953-23acaf1da7d0",
   "metadata": {},
   "source": [
    "1. A solução criada nesse projeto deve ser disponibilizada em repositório git e disponibilizada em servidor de repositórios (Github (recomendado), Bitbucket ou Gitlab). O projeto deve obedecer o Framework TDSP da Microsoft. Todos os artefatos produzidos deverão conter informações referentes a esse projeto (não serão aceitos documentos vazios ou fora de contexto). Escreva o link para seu repositório.\n",
    "   * Link para o repositório no GitHub: https://github.com/sergioopereira/EngenhariaMachineLearning \n",
    "\n",
    "2. Iremos desenvolver um preditor de arremessos usando duas abordagens (regressão e classificação) para prever se o \"Black Mamba\" (apelido de Kobe) acertou ou errou a cesta.\n",
    "    Para começar o desenvolvimento, desenhe um diagrama que demonstra todas as etapas necessárias em um projeto de inteligência artificial desde a aquisição de dados, passando pela criação dos modelos, indo até a operação do modelo.\n",
    "    <img src=\"https://docs.microsoft.com/pt-br/azure/architecture/data-science-process/media/lifecycle/tdsp-lifecycle2.png\" width=\"800\" height=\"597\">\n",
    "\n",
    "3. Descreva a importância de implementar pipelines de desenvolvimento e produção numa solução de aprendizado de máquinas.\n",
    "\n",
    "4. Como as ferramentas Streamlit, MLFlow, PyCaret e Scikit-Learn auxiliam na construção dos pipelines descritos anteriormente? A resposta deve abranger os seguintes aspectos:\n",
    "a. Rastreamento de experimentos;\n",
    "b. Funções de treinamento;\n",
    "c. Monitoramento da saúde do modelo;\n",
    "d. Atualização de modelo;\n",
    "e. Provisionamento (Deployment).\n",
    "\n",
    "5. Com base no diagrama realizado na questão 2, aponte os artefatos que serão criados ao longo de um projeto. Para cada artefato, indique qual seu objetivo.\n",
    "\n",
    "6. Implemente o pipeline de processamento de dados com o mlflow, rodada (run) com o nome \"PreparacaoDados\":\n",
    "a. Os dados devem estar localizados em \"/Data/kobe_dataset.csv\"\n",
    "b. Observe que há dados faltantes na base de dados! As linhas que possuem dados faltantes devem ser desconsideradas. Você também irá filtrar os dados onde o valor de shot_type for igual à 2PT Field Goal. Ainda, para esse exercício serão apenas consideradas as colunas: \n",
    "i. lat\n",
    "ii. lng\n",
    "iii. minutes remaining\n",
    "iv. period\n",
    "v. playoffs\n",
    "vi. shot_distance\n",
    "A variável shot_made_flag será seu alvo, onde 0 indica que Kobe errou e 1 que a cesta foi realizada. O dataset resultante será armazenado na pasta \"/Data/processed/data_filtered.parquet\". Ainda sobre essa seleção, qual a dimensão resultante do dataset?\n",
    "c. Separe os dados em treino (80%) e teste (20 %) usando uma escolha aleatória e estratificada. Armazene os datasets resultantes em \"/Data/operalization/base_{train|test}.parquet . Explique como a escolha de treino e teste afetam o resultado do modelo final. Quais estratégias ajudam a minimizar os efeitos de viés de dados.\n",
    "d. Registre os parâmetros (% teste) e métricas (tamanho de cada base) no MlFlow\n",
    "\n",
    "7. Implementar o pipeline de treinamento do modelo com o Mlflow usando o nome \"Treinamento\"\n",
    "a. Com os dados separados para treinamento, treine um modelo com regressão logística do sklearn usando a biblioteca pyCaret.\n",
    "b. Registre a função custo \"log loss\" usando a base de teste\n",
    "c. Com os dados separados para treinamento, treine um modelo de classificação do sklearn usando a biblioteca pyCaret. A escolha do algoritmo de classificação é livre. Justifique sua escolha.\n",
    "d. Registre a função custo \"log loss\" e F1_score para esse novo modelo\n",
    "\n",
    "8. Registre o modelo de classificação e o disponibilize através do MLFlow através de API. Selecione agora os dados da base de dados original onde shot_type for igual à 3PT Field Goal (será uma nova base de dados) e através da biblioteca requests, aplique o modelo treinado. Publique uma tabela com os resultados obtidos e indique o novo log loss e f1_score.\n",
    "a. O modelo é aderente a essa nova base? Justifique.\n",
    "b. Descreva como podemos monitorar a saúde do modelo no cenário com e sem a disponibilidade da variável resposta para o modelo em operação\n",
    "c. Descreva as estratégias reativa e preditiva de retreinamento para o modelo em operação.\n",
    "\n",
    "9. Implemente um dashboard de monitoramento da operação usando Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f90b67-a207-4afd-bbe9-2661da9ba175",
   "metadata": {},
   "source": [
    "# Início do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dcf721-515b-4991-a66f-1495918cf4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from sklearn import linear_model, preprocessing, metrics, model_selection\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "if 'inline_rc' not in dir():\n",
    "    inline_rc = dict(mpl.rcParams)\n",
    "\n",
    "SEED = 10\n",
    "\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.WARN)\n",
    "# logger = logging.getLogger(__name__)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3aa5a-ea23-414f-b810-deb2674b5aab",
   "metadata": {},
   "source": [
    "# Reset do estilo de cores do matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff08cff-e345-400c-869e-ab223d9521dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset matplotlib\n",
    "\n",
    "mpl.rcParams.update(inline_rc)\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "mpl.rc('font', **font)\n",
    "lines = {'linewidth' : 3}\n",
    "mpl.rc('lines', **lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54d948-612c-49d2-b3d7-0888655ee954",
   "metadata": {},
   "source": [
    "# Preditor de arremessos Kobe Bryant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41ef4cc-d1d0-4c21-bc6e-242fb94eca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "registered_model_name = 'modelo_arremesso' # Define o nome do modelo\n",
    "min_precision = 0.7 # Define a precisão\n",
    "model_version = -1 # recuperar a ultima versao\n",
    "nexamples = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723061f-4bd4-406d-a753-fbe724ba52dc",
   "metadata": {},
   "source": [
    "# Experimento de Classificação de Arremessos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a36d90-f848-4508-8984-9e5acfc9e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O componente MLflow Tracking é uma API e interface do usuário para registrar parâmetros, versões de código, métricas e arquivos de saída \n",
    "# ao executar seu código de aprendizado de máquina e para visualizar os resultados posteriormente.\n",
    "# Muitos usuários também executam o MLflow em suas máquinas locais com um banco de dados compatível com SQLAlchemy : SQLite . \n",
    "# Nesse caso, os artefatos são armazenados no ./mlruns diretório local e as entidades MLflow são inseridas em um arquivo de banco de dados SQLite mlruns.db.\n",
    "\n",
    "# Configura o MLflow para usar o sqlite como repositorio\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n",
    "\n",
    "experiment_name = 'Classificador de Arremessos' # Define o nome do experimento\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name) # Recuperar um experimento pelo nome do experimento da loja de back-end\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name) # Cria um experimento.\n",
    "    experiment = mlflow.get_experiment(experiment_id) # Recuperar um experimento por experiment_id da loja de back-end\n",
    "experiment_id = experiment.experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f5eb76-7205-4054-92f7-9a758308378f",
   "metadata": {},
   "source": [
    "# Leitura dos Dados de Classificação de Arremessos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597cccee-8bd4-456a-adb3-8ae184698eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Bases de Dados ==\n",
      "data_kobe (16228, 6)\n",
      "data_operation (4057, 6)\n",
      "data_novelty (5412, 6)\n",
      "Columns: Index(['lat', 'lon', 'minutes_remaining', 'playoffs', 'shot_distance',\n",
      "       'shot_made_flag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# COLOCAR RUN DE LEITURA DE DADOS\n",
    "# PARAMETROS: top_features,\n",
    "# METRICS: SHAPE de cada base de dados\n",
    "# ARTIFACTS: nenhum\n",
    "\n",
    "top_features = ['lat','lon','minutes_remaining', 'playoffs','shot_distance']\n",
    "\n",
    "# Inicia uma nova execução do MLflow, definindo-a como a execução ativa na qual as métricas e os parâmetros serão registrados. \n",
    "# O valor de retorno pode ser usado como gerenciador de contexto dentro de um withbloco; \n",
    "# caso contrário, você deve chamar end_run() para encerrar a execução atual.\n",
    "\n",
    "with mlflow.start_run(\n",
    "    experiment_id=experiment_id, \n",
    "    run_name = 'PreparacaoDados'):\n",
    "    df_kobe = pd.read_csv('../Data/dataset_kobe.csv',sep=',')\n",
    "    df_kobe = df_kobe.dropna() # remove a linha que possua alguma coluna vazia\n",
    "    df_kobe['shot_made_flag'] = df_kobe['shot_made_flag'].astype(int) # converte a coluna em inteiro\n",
    "    kobe_target_col = 'shot_made_flag' # Define a coluna alvo\n",
    "    kobe_label_map = df_kobe[['shot_made_flag']].drop_duplicates() # \n",
    "    \n",
    "    # Copia todos os dados do dataframe principal selecionando as top_features e o shot_type\n",
    "    df_kobe = df_kobe[top_features + ['shot_type', kobe_target_col]].copy()\n",
    "\n",
    "    # Copia os dados do dataframe com as top_features selecionando os casos onde shot_type for igual a \"2PT Field Goal\"\n",
    "    # e remove a coluna shot_type\n",
    "    data_kobe    = df_kobe[df_kobe['shot_type'] == '2PT Field Goal'].copy().drop('shot_type', axis=1)\n",
    "    \n",
    "    # Separar parte do dataframe que contém shot_type for igual a \"2PT Field Goal\"para compor a base de operacao e treinamento\n",
    "    data_kobe, data_operation, ytrain, ytest = model_selection.train_test_split(\n",
    "        data_kobe, \n",
    "        data_kobe[kobe_target_col],\n",
    "        test_size=0.2)\n",
    "    data_kobe[kobe_target_col]     = ytrain\n",
    "    data_operation[kobe_target_col] = ytest\n",
    "    \n",
    "    # Copia os dados do dataframe com as top_features selecionando os casos onde shot_type for igual a \"3PT Field Goal\"\n",
    "    # e remove a coluna shot_type\n",
    "    data_novelty = df_kobe[df_kobe['shot_type'] == '3PT Field Goal'].copy().drop('shot_type', axis=1)\n",
    "    \n",
    "    # grava os dataframes em arquivos parquet .\n",
    "    data_novelty.to_parquet('modelo_arremesso_novidade.parquet')\n",
    "    data_operation.to_parquet('modelo_arremesso_operacao.parquet')\n",
    "    \n",
    "    # to_parquet - Esta função grava o dataframe como um arquivo parquet.\n",
    "    # O Apache Parquet fornece uma serialização colunar binária particionada para quadros de dados. \n",
    "    # Ele foi projetado para tornar a leitura e gravação de quadros de dados eficientes e para facilitar o compartilhamento de dados entre linguagens de análise de dados. \n",
    "    # O Parquet pode usar uma variedade de técnicas de compactação para reduzir o tamanho do arquivo o máximo possível, mantendo um bom desempenho de leitura.\n",
    "    \n",
    "    # LOG DE PARAMETROS DO MODELO\n",
    "    mlflow.log_param(\"top_features\", top_features)\n",
    "\n",
    "    # LOG DE METRICAS GLOBAIS\n",
    "    mlflow.log_metric(\"data_train\", data_kobe.shape[0])\n",
    "    mlflow.log_metric(\"data_operation\", data_operation.shape[0])\n",
    "    mlflow.log_metric(\"data_novelty\", data_novelty.shape[0]) \n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "print('== Bases de Dados ==')\n",
    "print(f'data_kobe {data_kobe.shape}')\n",
    "print(f'data_operation {data_operation.shape}')\n",
    "print(f'data_novelty {data_novelty.shape}')\n",
    "print(f'Columns: {data_kobe.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e17201-9f1d-48cf-b1d0-48521e4fc3ef",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b84fa5e-b7e5-42da-86dd-a231bdc758a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n",
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    }
   ],
   "source": [
    "import pycaret.classification as pc # Essa função inicializa o ambiente de treinamento e cria o pipeline de transformação.\n",
    "# COLOCAR RUN DE TREINAMENTO DE MODELOS\n",
    "# PARAMETROS: fold_strategy, fold, model_name, registered_model_name, cross_validation\n",
    "# METRICS: auto sklearn\n",
    "# ARTIFACTS: plots\n",
    "\n",
    "model_name = 'lr'\n",
    "probability_threshold = 0.5\n",
    "cross_validation = True\n",
    "fold_strategy = 'stratifiedkfold',\n",
    "fold = 10\n",
    "\n",
    "# train/test\n",
    "s = pc.setup(data = data_kobe, \n",
    "             target = kobe_target_col,\n",
    "             train_size=0.7,\n",
    "             silent = True,\n",
    "             fold_strategy = 'stratifiedkfold',\n",
    "             fold = fold,\n",
    "             log_experiment = True, \n",
    "             experiment_name = experiment_name, \n",
    "             log_plots = True\n",
    "            )\n",
    "bestmodel = pc.create_model(model_name,\n",
    "                            cross_validation = cross_validation, \n",
    "                            probability_threshold=probability_threshold)\n",
    "\n",
    "# Log do run, e nao do modelo respectivo\n",
    "classification_plots = [ 'auc','pr','confusion_matrix',\n",
    "#                          'error', 'class_report', \n",
    "                        'threshold',\n",
    "                         'learning','vc','feature',\n",
    "                       ]\n",
    "for plot_type in classification_plots:\n",
    "    print('=> Aplicando plot ', plot_type)\n",
    "    try:\n",
    "        artifact = pc.plot_model(bestmodel, plot=plot_type, save=True, use_train_data=False)\n",
    "        mlflow.log_artifact(artifact)\n",
    "    except:\n",
    "        print('=> Nao possivel plotar: ', plot_type )\n",
    "        continue\n",
    "\n",
    "pc.save_model(bestmodel, f'./{registered_model_name}') \n",
    "# Carrega novamente o pipeline + bestmodel\n",
    "model_pipe = pc.load_model(f'./{registered_model_name}')\n",
    "\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0bd866-474a-4ac1-b1e2-69da1790fb70",
   "metadata": {},
   "source": [
    "# Avaliar Precisão Mínima "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68882c02-4277-424a-9a0b-16fba58e9c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_27cd1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_27cd1_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_27cd1_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_27cd1_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_27cd1_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_27cd1_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_27cd1_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_27cd1_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_27cd1_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_27cd1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_27cd1_row0_col0\" class=\"data row0 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_27cd1_row0_col1\" class=\"data row0 col1\" >0.5796</td>\n",
       "      <td id=\"T_27cd1_row0_col2\" class=\"data row0 col2\" >0.5968</td>\n",
       "      <td id=\"T_27cd1_row0_col3\" class=\"data row0 col3\" >0.4899</td>\n",
       "      <td id=\"T_27cd1_row0_col4\" class=\"data row0 col4\" >0.5710</td>\n",
       "      <td id=\"T_27cd1_row0_col5\" class=\"data row0 col5\" >0.5274</td>\n",
       "      <td id=\"T_27cd1_row0_col6\" class=\"data row0 col6\" >0.1527</td>\n",
       "      <td id=\"T_27cd1_row0_col7\" class=\"data row0 col7\" >0.1542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2123d8d7610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Rejeitado o modelo com precisão 0.571 (min: 0.7)\n"
     ]
    }
   ],
   "source": [
    "# COLOCAR RUN APROVACAO DE MODELO\n",
    "# PARAMETROS: min_precision\n",
    "# METRICS: new_version, precision\n",
    "# ARTIFACTS: None\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name = 'AprovacaoModelo'):\n",
    "    pred_holdout = pc.predict_model(bestmodel)\n",
    "    pr = metrics.precision_score(pred_holdout[kobe_target_col], pred_holdout['Label'],)\n",
    "    if pr > min_precision:\n",
    "        print(f'=> Aceito o modelo com precisão {pr} (min: {min_precision})')\n",
    "        pred_holdout.to_parquet('modelo_arremesso_teste.parquet')\n",
    "        # Assinatura do Modelo Inferida pelo MLFlow\n",
    "        model_features = list(data_kobe.drop(kobe_target_col, axis=1).columns)\n",
    "        inf_signature = infer_signature(data_kobe[model_features], model_pipe.predict(data_kobe))\n",
    "        # Exemplo de entrada para o MLmodel\n",
    "        input_example = {x: data_kobe[x].values[:nexamples] for x in model_features}\n",
    "        # Log do pipeline de modelagem do sklearn e registrar como uma nova versao\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model_pipe,\n",
    "            artifact_path=\"sklearn-model\",\n",
    "            registered_model_name=registered_model_name,\n",
    "            signature = inf_signature,\n",
    "            input_example = input_example\n",
    "        )\n",
    "        # Criacao do cliente do servico MLFlow e atualizacao versao modelo\n",
    "        client = MlflowClient()\n",
    "        if model_version == -1:\n",
    "            model_version = client.get_latest_versions(registered_model_name)[-1].version\n",
    "        # Registrar o modelo como staging\n",
    "        client.transition_model_version_stage(\n",
    "            name=registered_model_name,\n",
    "            version=model_version, # Verificar com usuario qual versao\n",
    "            stage=\"Staging\"\n",
    "        )\n",
    "    else:\n",
    "        print(f'=> Rejeitado o modelo com precisão {pr} (min: {min_precision})')\n",
    "\n",
    "    # LOG DE PARAMETROS DO MODELO\n",
    "    mlflow.log_param(\"precisao_minima\", min_precision)\n",
    "\n",
    "    # LOG DE METRICAS GLOBAIS\n",
    "    mlflow.log_metric(\"new_version\", model_version)\n",
    "    mlflow.log_metric(\"precisao\", pr)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e6cc5-5e87-45e0-8bce-78ad3220beb5",
   "metadata": {},
   "source": [
    "# Serviço do Modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597f3308-d135-4df6-a440-5ad7d7092480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\click\\core.py:2322: FutureWarning: `--no-conda` is deprecated and will be removed in a future MLflow release. Use `--env-manager=local` instead.\n",
      "  value = self.callback(ctx, self, value)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\spereira\\AppData\\Local\\Programs\\Python\\Python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\spereira\\AppData\\Local\\Programs\\Python\\Python38\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"D:\\virtualenvs\\.eng_ml\\Scripts\\mlflow.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\click\\core.py\", line 1055, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\click\\core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\mlflow\\models\\cli.py\", line 68, in serve\n",
      "    return _get_flavor_backend(\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\mlflow\\models\\cli.py\", line 210, in _get_flavor_backend\n",
      "    underlying_model_uri = ModelsArtifactRepository.get_underlying_uri(model_uri)\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\mlflow\\store\\artifact\\models_artifact_repo.py\", line 54, in get_underlying_uri\n",
      "    (name, version) = get_model_name_and_version(client, uri)\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\mlflow\\store\\artifact\\utils\\models.py\", line 74, in get_model_name_and_version\n",
      "    return model_name, str(_get_latest_model_version(client, model_name, model_stage))\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\mlflow\\store\\artifact\\utils\\models.py\", line 28, in _get_latest_model_version\n",
      "    latest = client.get_latest_versions(name, None if stage is None else [stage])\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\mlflow\\tracking\\client.py\", line 2048, in get_latest_versions\n",
      "    return self._get_registry_client().get_latest_versions(name, stages)\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\mlflow\\tracking\\_model_registry\\client.py\", line 149, in get_latest_versions\n",
      "    return self.store.get_latest_versions(name, stages)\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\mlflow\\store\\model_registry\\sqlalchemy_store.py\", line 440, in get_latest_versions\n",
      "    sql_registered_model = self._get_registered_model(session, name)\n",
      "  File \"d:\\virtualenvs\\.eng_ml\\lib\\site-packages\\mlflow\\store\\model_registry\\sqlalchemy_store.py\", line 211, in _get_registered_model\n",
      "    raise MlflowException(\n",
      "mlflow.exceptions.MlflowException: Registered Model with name=modelo_arremesso not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'sqlite:///mlruns.db'\n",
    "!mlflow models serve -m \"models:/modelo_arremesso/Staging\" --no-conda -p 5001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280fa332-5494-4b8f-988b-222a711f5c8e",
   "metadata": {},
   "source": [
    "# Operação Sistema preditor de arremessos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfce87eb-1eb4-4ebe-947b-eb74b3afb498",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m http_data \u001b[38;5;241m=\u001b[39m data_operation\u001b[38;5;241m.\u001b[39mdrop(kobe_target_col,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto_json(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39murl, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mhttp_data)\n\u001b[1;32m---> 10\u001b[0m data_operation\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moperation_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m data_operation\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodelo_arremesso_operacao.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m data_operation[data_operation\u001b[38;5;241m.\u001b[39moperation_label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshot_distance\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\util\\_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\io\\json\\_json.py:612\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m json_reader:\n\u001b[1;32m--> 612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\io\\json\\_json.py:746\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    744\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\io\\json\\_json.py:768\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    766\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 768\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\io\\json\\_json.py:880\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_numpy()\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 880\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_no_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\io\\json\\_json.py:1132\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1129\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1136\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1139\u001b[0m     }\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32md:\\virtualenvs\\.eng_ml\\lib\\site-packages\\pandas\\core\\internals\\construction.py:664\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m have_series:\n\u001b[0;32m    667\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "host = 'localhost'\n",
    "port = '5001'\n",
    "url = f'http://{host}:{port}/invocations'\n",
    "headers = {'Content-Type': 'application/json',}\n",
    "\n",
    "http_data = data_operation.drop(kobe_target_col,axis=1).to_json(orient='split')\n",
    "r = requests.post(url=url, headers=headers, data=http_data)\n",
    "\n",
    "data_operation.loc[:, 'operation_label'] = pd.read_json(r.text).values[:,0]\n",
    "\n",
    "data_operation.to_parquet('modelo_arremesso_operacao.parquet')\n",
    "data_operation[data_operation.operation_label == 1].sort_values('shot_distance', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835cc065-0861-48dc-a615-ca9f73384e81",
   "metadata": {},
   "source": [
    "# Revalidacao de Amostras para Monitoramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3b284-9826-45d6-8241-9db127483458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLOCAR RUN REVALIDACAO\n",
    "# PARAMETROS: min_samples_control\n",
    "# METRICS: matriz de confusao\n",
    "# ARTIFACTS:\n",
    "\n",
    "\n",
    "# Utilizacao da amostra de controle, que teria sido reavaliada por especialistas\n",
    "min_samples_control = 150\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name = 'RevalidacaoOperacao'):\n",
    "\n",
    "    data_operation = pd.read_parquet('modelo_arremesso_operacao.parquet')\n",
    "\n",
    "    data_control = data_operation.sample(min_samples_control, random_state=SEED)\n",
    "    data_control.to_parquet('modelo_arremesso_controle.parquet')\n",
    "\n",
    "    print('== DADOS DE CONTROLE ==')\n",
    "    print(metrics.classification_report(data_control[kobe_target_col], data_control['operation_label']))\n",
    "    \n",
    "    # LOG DE PARAMETROS DO MODELO\n",
    "    mlflow.log_param(\"min_samples_control\", min_samples_control)\n",
    "\n",
    "    # LOG DE METRICAS GLOBAIS\n",
    "    cm = metrics.confusion_matrix(data_control[kobe_target_col], data_control['operation_label'])\n",
    "    specificity = cm[0,0] / cm.sum(axis=1)[0]\n",
    "    sensibility = cm[1,1] / cm.sum(axis=1)[1]\n",
    "    precision   = cm[1,1] / cm.sum(axis=0)[1]\n",
    "    mlflow.log_metric(\"Especificidade\", specificity)\n",
    "    mlflow.log_metric(\"Sensibilidade\", sensibility)\n",
    "    mlflow.log_metric(\"Precisao\", precision)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3567a-d1ac-43b9-8417-3a0a2e9df90d",
   "metadata": {},
   "source": [
    "## Alarme de Desvio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f079fd3-eef6-441a-b1cc-886b8c850874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset matplotlib\n",
    "\n",
    "mpl.rcParams.update(inline_rc)\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "mpl.rc('font', **font)\n",
    "lines = {'linewidth' : 3}\n",
    "mpl.rc('lines', **lines)\n",
    "\n",
    "def data_drift_alarm(var_name, dev_data, data_test, data_control):    \n",
    "    sn.kdeplot(dev_data[var_name], label='Desenvolvimento')\n",
    "    sn.kdeplot(data_test[var_name], label='Teste')\n",
    "    sn.kdeplot(data_control[var_name], label='Monitoramento')\n",
    "    plt.grid()\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(f'Distribuição Variável {var_name}')\n",
    "    plt.ylabel('Densidade')\n",
    "    plt.xlabel(f'Unidade de {var_name}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0c376-9214-4bb6-84e5-d37518c2686f",
   "metadata": {},
   "source": [
    "## Alarme de Retreinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5faf0c8-5a31-4b2c-8708-8af4e9a2acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alarm(data_monitoring, testset, min_eff_alarm):\n",
    "    cm = metrics.confusion_matrix(data_monitoring[kobe_target_col], data_monitoring['operation_label'])\n",
    "    specificity_m = cm[0,0] / cm.sum(axis=1)[0]\n",
    "    sensibility_m = cm[1,1] / cm.sum(axis=1)[1]\n",
    "    precision_m   = cm[1,1] / cm.sum(axis=0)[1]\n",
    "\n",
    "    cm = metrics.confusion_matrix(testset[kobe_target_col], testset['Label'])\n",
    "    specificity_t = cm[0,0] / cm.sum(axis=1)[0]\n",
    "    sensibility_t = cm[1,1] / cm.sum(axis=1)[1]\n",
    "    precision_t   = cm[1,1] / cm.sum(axis=0)[1]\n",
    "\n",
    "    retrain = False\n",
    "    for name, metric_m, metric_t in zip(['especificidade', 'sensibilidade', 'precisao'],\n",
    "                                        [specificity_m, sensibility_m, precision_m],\n",
    "                                        [specificity_t, sensibility_t, precision_t]):\n",
    "        \n",
    "        print(f'\\t=> {name} de teste {metric_t} e de controle {metric_m}')\n",
    "        if (metric_t-metric_m)/metric_t > min_eff_alarm:\n",
    "            print(f'\\t=> MODELO OPERANDO FORA DO ESPERADO')\n",
    "            retrain = True\n",
    "        else:\n",
    "            print(f'\\t=> MODELO OPERANDO DENTRO DO ESPERADO')\n",
    "           \n",
    "        \n",
    "    return (retrain, [specificity_m, sensibility_m, precision_m],\n",
    "                                        [specificity_t, sensibility_t, precision_t] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49e194-1908-45fa-9c72-0e6456c1c87e",
   "metadata": {},
   "source": [
    "### Monitoramento Base Operacao "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d7c0f-dc12-4d3b-9d05-a65434321a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLOCAR RUN MONITORAMENTO OPERACAO\n",
    "# PARAMETROS: min_eff_alarm\n",
    "# METRICS: metricas avaliadas e de referencia\n",
    "# ARTIFACTS:\n",
    "\n",
    "print('== ALARME DE RETREINAMENTO - BASE CONTROLE ==')\n",
    "# 10% de desvio aceitavel na metrica. Deve ser estimado pelo conjunto de validacao cruzada. \n",
    "min_eff_alarm = 0.1 \n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name = 'MonitoramentoOperacao'):\n",
    "    data_control = pd.read_parquet('modelo_arremesso_controle.parquet')\n",
    "    \n",
    "    (retrain, [specificity_m, sensibility_m, precision_m],\n",
    "              [specificity_t, sensibility_t, precision_t] ) = alarm(data_control, pred_holdout, min_eff_alarm)\n",
    "    if retrain:\n",
    "        print('==> RETREINAMENTO NECESSARIO')\n",
    "    else:\n",
    "        print('==> RETREINAMENTO NAO NECESSARIO')\n",
    "    # LOG DE PARAMETROS DO MODELO\n",
    "    mlflow.log_param(\"min_eff_alarm\", min_eff_alarm)\n",
    "\n",
    "    # LOG DE METRICAS GLOBAIS\n",
    "    mlflow.log_metric(\"Alarme Retreino\", float(retrain))\n",
    "    mlflow.log_metric(\"Especificidade Controle\", specificity_m)\n",
    "    mlflow.log_metric(\"Sensibilidade Controle\", sensibility_m)\n",
    "    mlflow.log_metric(\"Precisao Controle\", precision_m)\n",
    "    mlflow.log_metric(\"Especificidade Teste\", specificity_t)\n",
    "    mlflow.log_metric(\"Sensibilidade Teste\", sensibility_t)\n",
    "    mlflow.log_metric(\"Precisao Teste\", precision_t)\n",
    "    \n",
    "    # LOG ARTEFATO\n",
    "    var_name = 'volatile acidity' # 'alcohol'\n",
    "    data_drift_alarm(var_name, data_kobe, pred_holdout, data_control)\n",
    "    plot_path = f'monitor_datadrift_{var_name}.png'\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    \n",
    "\n",
    "mlflow.end_run()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753f39e-ba23-4bab-b4f6-734295d46fe5",
   "metadata": {},
   "source": [
    "# Operação Aplicação Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb23c4-06fa-4d3a-9e91-fd9a1e03b70b",
   "metadata": {},
   "source": [
    "### Monitoramento Base de Novidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd34bf-0e10-498f-b522-76f9664268ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLOCAR RUN MONITORAMENTO NOVIDADE\n",
    "# PARAMETROS: min_eff_alarm\n",
    "# METRICS: metricas avaliadas e de referencia\n",
    "# ARTIFACTS:\n",
    "\n",
    "print('== ALARME DE RETREINAMENTO - BASE NOVIDADE ==')\n",
    "# 10% de desvio aceitavel na metrica. Deve ser estimado pelo conjunto de validacao cruzada. \n",
    "min_eff_alarm = 0.1 \n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name = 'MonitoramentoNovidade'):\n",
    "\n",
    "    model_uri = f\"models:/modelo_arremesso_tinto/Staging\"\n",
    "    loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "    data_novelty = pd.read_parquet('modelo_arremesso_novidade.parquet')\n",
    "    data_novelty.loc[:, 'operation_label'] = loaded_model.predict(data_novelty.drop(kobe_target_col,axis=1))\n",
    "    \n",
    "    (retrain, [specificity_m, sensibility_m, precision_m],\n",
    "              [specificity_t, sensibility_t, precision_t] ) = alarm(data_novelty, pred_holdout, min_eff_alarm)\n",
    "    if retrain:\n",
    "        print('==> RETREINAMENTO NECESSARIO')\n",
    "    else:\n",
    "        print('==> RETREINAMENTO NAO NECESSARIO')\n",
    "    # LOG DE PARAMETROS DO MODELO\n",
    "    mlflow.log_param(\"min_eff_alarm\", min_eff_alarm)\n",
    "\n",
    "    # LOG DE METRICAS GLOBAIS\n",
    "    mlflow.log_metric(\"Alarme Retreino\", float(retrain))\n",
    "    mlflow.log_metric(\"Especificidade Controle\", specificity_m)\n",
    "    mlflow.log_metric(\"Sensibilidade Controle\", sensibility_m)\n",
    "    mlflow.log_metric(\"Precisao Controle\", precision_m)\n",
    "    mlflow.log_metric(\"Especificidade Teste\", specificity_t)\n",
    "    mlflow.log_metric(\"Sensibilidade Teste\", sensibility_t)\n",
    "    mlflow.log_metric(\"Precisao Teste\", precision_t)\n",
    "    # LOG ARTEFATO\n",
    "    var_name = 'volatile acidity' # 'alcohol'\n",
    "    data_drift_alarm(var_name, data_kobe, pred_holdout, data_novelty)\n",
    "    plot_path = f'novidade_datadrift_{var_name}.png'\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "\n",
    "mlflow.end_run() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
